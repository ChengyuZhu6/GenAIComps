# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# this file should be run in the root of the repo
# images used by GenAIExamples: llm-tgi,llm-ollama,llm-docsum-tgi,llm-faqgen-tgi,llm-vllm,llm-vllm-ray
services:
  llm-tgi:
    build:
      dockerfile: comps/llms/text-generation/tgi/Dockerfile
    image: ${REGISTRY}opea/llm-tgi:${TAG:-latest}
  llm-ollama:
    build:
      dockerfile: comps/llms/text-generation/ollama/Dockerfile
    image: ${REGISTRY}opea/llm-ollama:${TAG:-latest}
  llm-docsum-tgi:
    build:
      dockerfile: comps/llms/summarization/tgi/Dockerfile
    image: ${REGISTRY}opea/llm-docsum-tgi:${TAG:-latest}
  llm-faqgen-tgi:
    build:
      dockerfile: comps/llms/faq-generation/tgi/Dockerfile
    image: ${REGISTRY}opea/llm-faqgen-tgi:${TAG:-latest}
  llm-vllm:
    build:
      dockerfile: comps/llms/text-generation/vllm/docker/Dockerfile.microservice
    image: ${REGISTRY}opea/llm-vllm:${TAG:-latest}
  llm-vllm-ray:
    build:
      dockerfile: comps/llms/text-generation/vllm-ray/docker/Dockerfile.microservice
    image: ${REGISTRY}opea/llm-vllm-ray:${TAG:-latest}
  vllm_ray:
    build:
      dockerfile: comps/llms/text-generation/vllm-ray/docker/Dockerfile.vllmray
    image: ${REGISTRY}opea/vllm_ray:${TAG:-latest}
  vllm:
    build:
      dockerfile: comps/llms/text-generation/vllm/docker/Dockerfile.hpu
    image: ${REGISTRY}opea/vllm:${TAG:-latest}
  llm-vllm-xft:
    build:
      dockerfile: comps/llms/text-generation/vllm-xft/docker/Dockerfile
    image: ${REGISTRY}opea/llm-vllm-xft:${TAG:-latest}
  lm-eval:
    build:
      dockerfile: comps/llms/utils/lm-eval/Dockerfile.cpu
    image: ${REGISTRY}opea/lm-eval:${TAG:-latest}
