# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

version: "3.8"

services:
  vllm-service:
    image: intelanalytics/ipex-llm-serving-cpu:latest
    container_name: vllm-cpu-server
    mem_limit: 64g
    cpuset: "0-47"
    ports:
      - "8008:80"
    volumes:
      - "/root/.cache/modelscope/hub/qwen/Qwen2-7B:/data"
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      LLM_MODEL: Qwen2-7B
    ipc: host
    command: -c "export VLLM_CPU_KVCACHE_SPACE=40 VLLM_LOGGING_LEVEL=DEBUG && python3 -m vllm.entrypoints.openai.api_server --enforce-eager --model /data --tensor-parallel-size 1 --host 0.0.0.0 --port 80 --trust-remote-code --served-model-name Qwen2-7B"
  llm:
    image: quay.io/chengyu_zhu/opea/llm-vllm-ipex:latest
    container_name: llm-vllm-cpu-server
    depends_on:
      - vllm-service
    ports:
      - "9090:9000"
    ipc: host
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      vLLM_ENDPOINT: http://10.67.124.46:8008
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      LLM_MODEL: Qwen2-7B
      LOGFLAG: True
    restart: unless-stopped

networks:
  default:
    driver: bridge
